{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMDNiQX7uJQ1fESQdXBV/ym"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NW5vmX1u8NvJ","executionInfo":{"status":"ok","timestamp":1740705241876,"user_tz":300,"elapsed":1335,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"dbd7ad1e-730b-4415-aa54-b712e670a82d"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/root/.kaggle’: File exists\n","Dataset URL: https://www.kaggle.com/datasets/manishkc06/usa-census-income-data\n","License(s): unknown\n","Downloading usa-census-income-data.zip to /content\n","  0% 0.00/6.24M [00:00<?, ?B/s]\n","100% 6.24M/6.24M [00:00<00:00, 196MB/s]\n"]}],"source":["# Modulos y datos\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Machine Learning\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import metrics\n","\n","# Configuremos el acceso a la infraestructura de kaggle\n","!mkdir ~/.kaggle\n","!touch ~/.kaggle/kaggle.json\n","\n","# Definir un diccionario para almacenar el nombre del usuario y la llave de acceso\n","api_token_26feb2025 = {\"username\":\"robintux\",\"key\":\"ca9634802e098162a09e1006da3dbe94\"}\n","\n","import json\n","with open(\"/root/.kaggle/kaggle.json\", \"w\") as file:\n","  json.dump(api_token_26feb2025, file)\n","\n","# Asignemos unos permisos adecuados al archivo kaggle.json\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","# Dataset\n","!kaggle datasets download manishkc06/usa-census-income-data"]},{"cell_type":"code","source":["# Descromprimimos el archivo descargado de kaggle\n","!unzip usa-census-income-data.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xd_zYDbj9UbC","executionInfo":{"status":"ok","timestamp":1740705242550,"user_tz":300,"elapsed":666,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"12d3295c-de28-4465-acb2-d91e1689ffab"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  usa-census-income-data.zip\n","  inflating: Training_set_census.csv  \n"]}]},{"cell_type":"code","source":["# Cargamos el dataset en memoria ram\n","income = pd.read_csv(\"Training_set_census.csv\")"],"metadata":{"id":"lmc0Ft1t9jtt","executionInfo":{"status":"ok","timestamp":1740705244381,"user_tz":300,"elapsed":1827,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["income.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sQkADkaT9vLG","executionInfo":{"status":"ok","timestamp":1740705244666,"user_tz":300,"elapsed":282,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"bda9d91b-a75c-4e8a-af60-fe3185f2c7f7"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 200000 entries, 0 to 199999\n","Data columns (total 41 columns):\n"," #   Column                            Non-Null Count   Dtype \n","---  ------                            --------------   ----- \n"," 0   age                               200000 non-null  int64 \n"," 1   class_of_worker                   200000 non-null  object\n"," 2   industry_code                     200000 non-null  int64 \n"," 3   occupation_code                   200000 non-null  int64 \n"," 4   education                         200000 non-null  object\n"," 5   wage_per_hour                     200000 non-null  int64 \n"," 6   enrolled_in_edu_inst_lastwk       200000 non-null  object\n"," 7   marital_status                    200000 non-null  object\n"," 8   major_industry_code               200000 non-null  object\n"," 9   major_occupation_code             200000 non-null  object\n"," 10  race                              200000 non-null  object\n"," 11  hispanic_origin                   199408 non-null  object\n"," 12  sex                               200000 non-null  object\n"," 13  member_of_labor_union             200000 non-null  object\n"," 14  reason_for_unemployment           200000 non-null  object\n"," 15  full_parttime_employment_stat     200000 non-null  object\n"," 16  capital_gains                     200000 non-null  int64 \n"," 17  capital_losses                    200000 non-null  int64 \n"," 18  dividend_from_Stocks              200000 non-null  int64 \n"," 19  tax_filer_status                  200000 non-null  object\n"," 20  region_of_previous_residence      200000 non-null  object\n"," 21  state_of_previous_residence       199523 non-null  object\n"," 22  d_household_family_stat           200000 non-null  object\n"," 23  d_household_summary               200000 non-null  object\n"," 24  migration_msa                     133327 non-null  object\n"," 25  migration_reg                     133327 non-null  object\n"," 26  migration_within_reg              133327 non-null  object\n"," 27  live_1_year_ago                   200000 non-null  object\n"," 28  migration_sunbelt                 133327 non-null  object\n"," 29  num_person_Worked_employer        200000 non-null  int64 \n"," 30  family_members_under_18           200000 non-null  object\n"," 31  country_father                    195478 non-null  object\n"," 32  country_mother                    195864 non-null  object\n"," 33  country_self                      197688 non-null  object\n"," 34  citizenship                       200000 non-null  object\n"," 35  business_or_self_employed         200000 non-null  int64 \n"," 36  fill_questionnaire_veteran_admin  200000 non-null  object\n"," 37  veterans_benefits                 200000 non-null  int64 \n"," 38  weeks_worked_in_year              200000 non-null  int64 \n"," 39  year                              200000 non-null  int64 \n"," 40  income_level                      200000 non-null  int64 \n","dtypes: int64(13), object(28)\n","memory usage: 62.6+ MB\n"]}]},{"cell_type":"code","source":["# Valores faltantes\n","income.isnull().sum().sort_values(ascending = False)*100/income.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KOHX8J2p-S1-","executionInfo":{"status":"ok","timestamp":1740705244953,"user_tz":300,"elapsed":285,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"99906949-a627-4177-e1a1-27c1b3f6e883"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["migration_msa                       33.3365\n","migration_reg                       33.3365\n","migration_within_reg                33.3365\n","migration_sunbelt                   33.3365\n","country_father                       2.2610\n","country_mother                       2.0680\n","country_self                         1.1560\n","hispanic_origin                      0.2960\n","state_of_previous_residence          0.2385\n","num_person_Worked_employer           0.0000\n","d_household_summary                  0.0000\n","live_1_year_ago                      0.0000\n","age                                  0.0000\n","family_members_under_18              0.0000\n","citizenship                          0.0000\n","business_or_self_employed            0.0000\n","fill_questionnaire_veteran_admin     0.0000\n","veterans_benefits                    0.0000\n","weeks_worked_in_year                 0.0000\n","year                                 0.0000\n","d_household_family_stat              0.0000\n","region_of_previous_residence         0.0000\n","class_of_worker                      0.0000\n","major_occupation_code                0.0000\n","industry_code                        0.0000\n","occupation_code                      0.0000\n","education                            0.0000\n","wage_per_hour                        0.0000\n","enrolled_in_edu_inst_lastwk          0.0000\n","marital_status                       0.0000\n","major_industry_code                  0.0000\n","race                                 0.0000\n","tax_filer_status                     0.0000\n","sex                                  0.0000\n","member_of_labor_union                0.0000\n","reason_for_unemployment              0.0000\n","full_parttime_employment_stat        0.0000\n","capital_gains                        0.0000\n","capital_losses                       0.0000\n","dividend_from_Stocks                 0.0000\n","income_level                         0.0000\n","dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>migration_msa</th>\n","      <td>33.3365</td>\n","    </tr>\n","    <tr>\n","      <th>migration_reg</th>\n","      <td>33.3365</td>\n","    </tr>\n","    <tr>\n","      <th>migration_within_reg</th>\n","      <td>33.3365</td>\n","    </tr>\n","    <tr>\n","      <th>migration_sunbelt</th>\n","      <td>33.3365</td>\n","    </tr>\n","    <tr>\n","      <th>country_father</th>\n","      <td>2.2610</td>\n","    </tr>\n","    <tr>\n","      <th>country_mother</th>\n","      <td>2.0680</td>\n","    </tr>\n","    <tr>\n","      <th>country_self</th>\n","      <td>1.1560</td>\n","    </tr>\n","    <tr>\n","      <th>hispanic_origin</th>\n","      <td>0.2960</td>\n","    </tr>\n","    <tr>\n","      <th>state_of_previous_residence</th>\n","      <td>0.2385</td>\n","    </tr>\n","    <tr>\n","      <th>num_person_Worked_employer</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>d_household_summary</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>live_1_year_ago</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>age</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>family_members_under_18</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>citizenship</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>business_or_self_employed</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>fill_questionnaire_veteran_admin</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>veterans_benefits</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>weeks_worked_in_year</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>year</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>d_household_family_stat</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>region_of_previous_residence</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>class_of_worker</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>major_occupation_code</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>industry_code</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>occupation_code</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>education</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>wage_per_hour</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>enrolled_in_edu_inst_lastwk</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>marital_status</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>major_industry_code</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>race</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>tax_filer_status</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>sex</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>member_of_labor_union</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>reason_for_unemployment</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>full_parttime_employment_stat</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>capital_gains</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>capital_losses</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>dividend_from_Stocks</th>\n","      <td>0.0000</td>\n","    </tr>\n","    <tr>\n","      <th>income_level</th>\n","      <td>0.0000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["# Que hacer con los valores faltantes\n","\n","  # Elimino todas las columnas con valores faltantes\n","\n","  # Eliminar las columnas que tienen mas del 30% de valores faltantes y utilizar algunas estrategia para\n","  # rellenar los valores faltantes del resto de columnas\n","\n",""],"metadata":{"id":"xT0LUTGW-7He","executionInfo":{"status":"ok","timestamp":1740705244959,"user_tz":300,"elapsed":4,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Implementemos la primera estrategia : Elimino todas las columnas con valores faltantes\n","mv = income.isnull().sum().sort_values(ascending = False)*100/income.shape[0]\n","lista_columnas_eliminar1 = list(mv[mv != 0 ].index)\n","income_estra1 = income.drop(lista_columnas_eliminar1, axis = 1)\n","income_estra1.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8hx9i1lt_fog","executionInfo":{"status":"ok","timestamp":1740705245462,"user_tz":300,"elapsed":499,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"c7637313-4b3a-463e-a951-84a1f47d58be"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 200000 entries, 0 to 199999\n","Data columns (total 32 columns):\n"," #   Column                            Non-Null Count   Dtype \n","---  ------                            --------------   ----- \n"," 0   age                               200000 non-null  int64 \n"," 1   class_of_worker                   200000 non-null  object\n"," 2   industry_code                     200000 non-null  int64 \n"," 3   occupation_code                   200000 non-null  int64 \n"," 4   education                         200000 non-null  object\n"," 5   wage_per_hour                     200000 non-null  int64 \n"," 6   enrolled_in_edu_inst_lastwk       200000 non-null  object\n"," 7   marital_status                    200000 non-null  object\n"," 8   major_industry_code               200000 non-null  object\n"," 9   major_occupation_code             200000 non-null  object\n"," 10  race                              200000 non-null  object\n"," 11  sex                               200000 non-null  object\n"," 12  member_of_labor_union             200000 non-null  object\n"," 13  reason_for_unemployment           200000 non-null  object\n"," 14  full_parttime_employment_stat     200000 non-null  object\n"," 15  capital_gains                     200000 non-null  int64 \n"," 16  capital_losses                    200000 non-null  int64 \n"," 17  dividend_from_Stocks              200000 non-null  int64 \n"," 18  tax_filer_status                  200000 non-null  object\n"," 19  region_of_previous_residence      200000 non-null  object\n"," 20  d_household_family_stat           200000 non-null  object\n"," 21  d_household_summary               200000 non-null  object\n"," 22  live_1_year_ago                   200000 non-null  object\n"," 23  num_person_Worked_employer        200000 non-null  int64 \n"," 24  family_members_under_18           200000 non-null  object\n"," 25  citizenship                       200000 non-null  object\n"," 26  business_or_self_employed         200000 non-null  int64 \n"," 27  fill_questionnaire_veteran_admin  200000 non-null  object\n"," 28  veterans_benefits                 200000 non-null  int64 \n"," 29  weeks_worked_in_year              200000 non-null  int64 \n"," 30  year                              200000 non-null  int64 \n"," 31  income_level                      200000 non-null  int64 \n","dtypes: int64(13), object(19)\n","memory usage: 48.8+ MB\n"]}]},{"cell_type":"code","source":["# Implementemos la segunda estrategia para trabajar/procesar los valores faltantes\n","lista_columnas_eliminar2 = list(mv[mv > 30].index)\n","income_estra2 = income.drop(lista_columnas_eliminar2, axis = 1)\n","rellenar_na = income_estra2.isnull().sum().sort_values(ascending= False)\n","# income_estra2[list(rellenar_na[rellenar_na!= 0].index)].info()\n","\n","for col in list(rellenar_na[rellenar_na!= 0].index):\n","  income_estra2[col] = income_estra2[col].fillna(value =income_estra2[col].mode()[0] )\n","\n","income_estra2.isnull().sum().sum()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xT6AMECVARQC","executionInfo":{"status":"ok","timestamp":1740705246166,"user_tz":300,"elapsed":702,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"77663bdb-153b-48d1-b214-e6cfd7e8464e"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["income_estra2.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdN4IQEMDAMe","executionInfo":{"status":"ok","timestamp":1740705246412,"user_tz":300,"elapsed":243,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"8f3d34c7-9db6-406d-dfbf-960a01f979bd"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 200000 entries, 0 to 199999\n","Data columns (total 37 columns):\n"," #   Column                            Non-Null Count   Dtype \n","---  ------                            --------------   ----- \n"," 0   age                               200000 non-null  int64 \n"," 1   class_of_worker                   200000 non-null  object\n"," 2   industry_code                     200000 non-null  int64 \n"," 3   occupation_code                   200000 non-null  int64 \n"," 4   education                         200000 non-null  object\n"," 5   wage_per_hour                     200000 non-null  int64 \n"," 6   enrolled_in_edu_inst_lastwk       200000 non-null  object\n"," 7   marital_status                    200000 non-null  object\n"," 8   major_industry_code               200000 non-null  object\n"," 9   major_occupation_code             200000 non-null  object\n"," 10  race                              200000 non-null  object\n"," 11  hispanic_origin                   200000 non-null  object\n"," 12  sex                               200000 non-null  object\n"," 13  member_of_labor_union             200000 non-null  object\n"," 14  reason_for_unemployment           200000 non-null  object\n"," 15  full_parttime_employment_stat     200000 non-null  object\n"," 16  capital_gains                     200000 non-null  int64 \n"," 17  capital_losses                    200000 non-null  int64 \n"," 18  dividend_from_Stocks              200000 non-null  int64 \n"," 19  tax_filer_status                  200000 non-null  object\n"," 20  region_of_previous_residence      200000 non-null  object\n"," 21  state_of_previous_residence       200000 non-null  object\n"," 22  d_household_family_stat           200000 non-null  object\n"," 23  d_household_summary               200000 non-null  object\n"," 24  live_1_year_ago                   200000 non-null  object\n"," 25  num_person_Worked_employer        200000 non-null  int64 \n"," 26  family_members_under_18           200000 non-null  object\n"," 27  country_father                    200000 non-null  object\n"," 28  country_mother                    200000 non-null  object\n"," 29  country_self                      200000 non-null  object\n"," 30  citizenship                       200000 non-null  object\n"," 31  business_or_self_employed         200000 non-null  int64 \n"," 32  fill_questionnaire_veteran_admin  200000 non-null  object\n"," 33  veterans_benefits                 200000 non-null  int64 \n"," 34  weeks_worked_in_year              200000 non-null  int64 \n"," 35  year                              200000 non-null  int64 \n"," 36  income_level                      200000 non-null  int64 \n","dtypes: int64(13), object(24)\n","memory usage: 56.5+ MB\n"]}]},{"cell_type":"code","source":["# Transformar la informacion de naturaleza cualitativa a informacion de naturaleza cuantitativa\n","cols_object = income_estra1.select_dtypes([\"object\"]).columns\n","list(cols_object)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7UqzzPNkDpJk","executionInfo":{"status":"ok","timestamp":1740705246456,"user_tz":300,"elapsed":41,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"28f38a30-0030-4984-fc97-0f0e9b4b5c8d"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['class_of_worker',\n"," 'education',\n"," 'enrolled_in_edu_inst_lastwk',\n"," 'marital_status',\n"," 'major_industry_code',\n"," 'major_occupation_code',\n"," 'race',\n"," 'sex',\n"," 'member_of_labor_union',\n"," 'reason_for_unemployment',\n"," 'full_parttime_employment_stat',\n"," 'tax_filer_status',\n"," 'region_of_previous_residence',\n"," 'd_household_family_stat',\n"," 'd_household_summary',\n"," 'live_1_year_ago',\n"," 'family_members_under_18',\n"," 'citizenship',\n"," 'fill_questionnaire_veteran_admin']"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["pd.get_dummies(income_estra1[\"class_of_worker\"],dtype= \"int\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":478},"id":"ysGujdNTEQWK","executionInfo":{"status":"ok","timestamp":1740705246540,"user_tz":300,"elapsed":79,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"9100d79f-0dc5-4bb5-9c25-cec50c1c8ffe"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         Federal government   Local government   Never worked  \\\n","0                         0                  0              0   \n","1                         0                  0              0   \n","2                         0                  0              0   \n","3                         0                  0              0   \n","4                         0                  0              0   \n","...                     ...                ...            ...   \n","199995                    0                  0              0   \n","199996                    0                  0              0   \n","199997                    0                  0              0   \n","199998                    0                  0              0   \n","199999                    0                  0              0   \n","\n","         Not in universe   Private   Self-employed-incorporated  \\\n","0                      0         0                            0   \n","1                      0         0                            0   \n","2                      0         0                            0   \n","3                      0         0                            0   \n","4                      0         0                            0   \n","...                  ...       ...                          ...   \n","199995                 0         0                            0   \n","199996                 0         0                            0   \n","199997                 0         0                            0   \n","199998                 0         0                            0   \n","199999                 0         0                            0   \n","\n","         Self-employed-not incorporated   State government   Without pay  \\\n","0                                     0                  0             0   \n","1                                     0                  0             0   \n","2                                     0                  0             0   \n","3                                     0                  0             0   \n","4                                     0                  0             0   \n","...                                 ...                ...           ...   \n","199995                                0                  0             0   \n","199996                                0                  0             0   \n","199997                                0                  0             0   \n","199998                                0                  0             0   \n","199999                                0                  0             0   \n","\n","        Federal government  Local government  Never worked  Not in universe  \\\n","0                        0                 0             0                0   \n","1                        0                 0             0                0   \n","2                        0                 0             0                0   \n","3                        0                 0             0                1   \n","4                        0                 0             0                1   \n","...                    ...               ...           ...              ...   \n","199995                   0                 0             0                1   \n","199996                   0                 0             0                0   \n","199997                   0                 0             0                1   \n","199998                   0                 0             0                0   \n","199999                   0                 0             0                1   \n","\n","        Private  Self-employed-incorporated  Self-employed-not incorporated  \\\n","0             1                           0                               0   \n","1             1                           0                               0   \n","2             1                           0                               0   \n","3             0                           0                               0   \n","4             0                           0                               0   \n","...         ...                         ...                             ...   \n","199995        0                           0                               0   \n","199996        1                           0                               0   \n","199997        0                           0                               0   \n","199998        0                           0                               0   \n","199999        0                           0                               0   \n","\n","        State government  Without pay  \n","0                      0            0  \n","1                      0            0  \n","2                      0            0  \n","3                      0            0  \n","4                      0            0  \n","...                  ...          ...  \n","199995                 0            0  \n","199996                 0            0  \n","199997                 0            0  \n","199998                 1            0  \n","199999                 0            0  \n","\n","[200000 rows x 18 columns]"],"text/html":["\n","  <div id=\"df-6b793000-f859-49bd-9bdf-28ce21620099\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Federal government</th>\n","      <th>Local government</th>\n","      <th>Never worked</th>\n","      <th>Not in universe</th>\n","      <th>Private</th>\n","      <th>Self-employed-incorporated</th>\n","      <th>Self-employed-not incorporated</th>\n","      <th>State government</th>\n","      <th>Without pay</th>\n","      <th>Federal government</th>\n","      <th>Local government</th>\n","      <th>Never worked</th>\n","      <th>Not in universe</th>\n","      <th>Private</th>\n","      <th>Self-employed-incorporated</th>\n","      <th>Self-employed-not incorporated</th>\n","      <th>State government</th>\n","      <th>Without pay</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>199995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>199999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200000 rows × 18 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b793000-f859-49bd-9bdf-28ce21620099')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6b793000-f859-49bd-9bdf-28ce21620099 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6b793000-f859-49bd-9bdf-28ce21620099');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-532b70b6-1a76-4b4d-9774-f2bb9ca9a719\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-532b70b6-1a76-4b4d-9774-f2bb9ca9a719')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-532b70b6-1a76-4b4d-9774-f2bb9ca9a719 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# Iniciamos con un dataframe en blanco\n","data_cuali = pd.DataFrame()\n","\n","# COnsideremos utilizar solo el 50% de los datos de income_estra1\n","income_estra1 = income_estra1.sample(frac = 0.5)\n","\n","\n","for col_obj in list(cols_object):\n","  data_cuali = pd.concat([data_cuali, pd.get_dummies(income_estra1[col_obj],dtype= \"int\")], axis = 1)\n","\n","data_cuali.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxs5oaLUE2pJ","executionInfo":{"status":"ok","timestamp":1740705258625,"user_tz":300,"elapsed":2110,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"516b123f-dde7-4f42-d0d4-51d841934878"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 100000 entries, 114188 to 4726\n","Columns: 341 entries,  Federal government to Yes\n","dtypes: int64(341)\n","memory usage: 260.9 MB\n"]}]},{"cell_type":"code","source":["# de income_estra1 debo remover las variables de tipo object\n","income_estra1 = income_estra1.drop(list(cols_object), axis = 1)\n","\n","# juntar/concatenar income_estra1 con data_cuali\n","income_estra1 = pd.concat([income_estra1, data_cuali], axis = 1)\n","\n","income_estra1.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHsUG5C9H00Y","executionInfo":{"status":"ok","timestamp":1740705467144,"user_tz":300,"elapsed":290,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"3fd0c0b7-630e-42ad-d4c2-9a8dff1e571e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Index: 100000 entries, 114188 to 4726\n","Columns: 354 entries, age to Yes\n","dtypes: int64(354)\n","memory usage: 270.8 MB\n"]}]},{"cell_type":"code","source":["# Guardemos en disco duro income_estra1\n","income_estra1.to_csv(\"income_estra1.csv\")"],"metadata":{"id":"v8-dYTmKIrF6","executionInfo":{"status":"ok","timestamp":1740705582959,"user_tz":300,"elapsed":7328,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Ya podemos construir un primer modelo regresional y un primer modelo\n","# de tipo arbol de decision\n","\n","# Definamos las variables independientes y la variable dependiente (income_level)\n","y = income_estra1.income_level\n","X = income_estra1.drop([\"income_level\"], axis = 1)\n","\n"],"metadata":{"id":"zFeODu16JDAj","executionInfo":{"status":"ok","timestamp":1740705794435,"user_tz":300,"elapsed":136,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Primer Paso : Particionar los datos\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size= 0.15)\n","\n","# Segundo Paso : Instanciamos las clases a modelar\n","model_base_reg = LogisticRegression(max_iter=10**4)\n","model_base_tree = DecisionTreeClassifier()\n","\n","# Tercer Paso : Ajustamos los modelos\n","model_base_reg.fit(X_train, y_train)\n","model_base_tree.fit(X_train, y_train)\n","\n","# Cuarto Paso : Etapa1\n","y_forecast_base_reg = model_base_reg.predict(X_test)\n","y_forecast_base_tree = model_base_tree.predict(X_test)\n","\n","# Cuarto Paso : Etapa2\n","acc_base_reg = metrics.accuracy_score(y_test, y_forecast_base_reg)\n","acc_base_tree = metrics.accuracy_score(y_test, y_forecast_base_tree)\n","\n","# Mostremos estos KPI calculados\n","print(\"\"\"\n","  Modelo de regresion logistica : %f\n","  Modelo de Arbol de Decision : %f\n","\n","\"\"\" %(acc_base_reg, acc_base_tree))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZt_pPqTJp5Y","executionInfo":{"status":"ok","timestamp":1740706942112,"user_tz":300,"elapsed":627519,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"6b94c80c-a3e4-4796-a896-39cff7dc8c3d"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Modelo de regresion logistica : 0.944133\n","  Modelo de Arbol de Decision : 0.924933\n","\n","\n"]}]},{"cell_type":"code","source":["# Analisis de la estabilidad del modelo : regresional"],"metadata":{"id":"kigEpe5-O3DT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analisis de la estabilidad del modelo : Arbol de decision"],"metadata":{"id":"uaopWoEWRT-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MOdificar y analizar el poder predicto de los arboles de decision considerando la\n","# profundidad\n"],"metadata":{"id":"Vfe3O-mGSCTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tambien construir modelos de regresion y de tipo arbol de decision con el\n","# dataset income_estra2"],"metadata":{"id":"H_BAJILnTrbu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bosques Aleatorios"],"metadata":{"id":"FOzFVz5NSu1G"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"6l1AdhxBSgwb","executionInfo":{"status":"ok","timestamp":1740708401973,"user_tz":300,"elapsed":12,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["income_estra2.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0ZWGyh_TloW","executionInfo":{"status":"ok","timestamp":1740708417478,"user_tz":300,"elapsed":795,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"5d476c35-fac2-4db0-f2bd-fe09b5e0cddb"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 200000 entries, 0 to 199999\n","Data columns (total 37 columns):\n"," #   Column                            Non-Null Count   Dtype \n","---  ------                            --------------   ----- \n"," 0   age                               200000 non-null  int64 \n"," 1   class_of_worker                   200000 non-null  object\n"," 2   industry_code                     200000 non-null  int64 \n"," 3   occupation_code                   200000 non-null  int64 \n"," 4   education                         200000 non-null  object\n"," 5   wage_per_hour                     200000 non-null  int64 \n"," 6   enrolled_in_edu_inst_lastwk       200000 non-null  object\n"," 7   marital_status                    200000 non-null  object\n"," 8   major_industry_code               200000 non-null  object\n"," 9   major_occupation_code             200000 non-null  object\n"," 10  race                              200000 non-null  object\n"," 11  hispanic_origin                   200000 non-null  object\n"," 12  sex                               200000 non-null  object\n"," 13  member_of_labor_union             200000 non-null  object\n"," 14  reason_for_unemployment           200000 non-null  object\n"," 15  full_parttime_employment_stat     200000 non-null  object\n"," 16  capital_gains                     200000 non-null  int64 \n"," 17  capital_losses                    200000 non-null  int64 \n"," 18  dividend_from_Stocks              200000 non-null  int64 \n"," 19  tax_filer_status                  200000 non-null  object\n"," 20  region_of_previous_residence      200000 non-null  object\n"," 21  state_of_previous_residence       200000 non-null  object\n"," 22  d_household_family_stat           200000 non-null  object\n"," 23  d_household_summary               200000 non-null  object\n"," 24  live_1_year_ago                   200000 non-null  object\n"," 25  num_person_Worked_employer        200000 non-null  int64 \n"," 26  family_members_under_18           200000 non-null  object\n"," 27  country_father                    200000 non-null  object\n"," 28  country_mother                    200000 non-null  object\n"," 29  country_self                      200000 non-null  object\n"," 30  citizenship                       200000 non-null  object\n"," 31  business_or_self_employed         200000 non-null  int64 \n"," 32  fill_questionnaire_veteran_admin  200000 non-null  object\n"," 33  veterans_benefits                 200000 non-null  int64 \n"," 34  weeks_worked_in_year              200000 non-null  int64 \n"," 35  year                              200000 non-null  int64 \n"," 36  income_level                      200000 non-null  int64 \n","dtypes: int64(13), object(24)\n","memory usage: 56.5+ MB\n"]}]},{"cell_type":"code","source":["# Con este dataset construir un modelo de regresion logistica (base)"],"metadata":{"id":"7CUqgQB5T6MM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construyamos nuestro primer bosque aleatorio\n","\n","# Primer paso : Particionamos los datos\n","X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)\n","\n","# Segundo Paso : Instanceamos la clase RandomForestClassifier\n","model_base_RF = RandomForestClassifier()\n","\n","# Tercer Paso : Ajuste del modelo\n","model_base_RF.fit(X_train, y_train)\n","\n","# Cuarto Paso : Etapa1\n","y_forecast_base_RF = model_base_RF.predict(X_test)\n","\n","# Cuarto Paso : Etapa2\n","acc_base_RF = metrics.accuracy_score(y_test, y_forecast_base_RF)\n","acc_base_RF\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HkcxpJViT9mh","executionInfo":{"status":"ok","timestamp":1740708808336,"user_tz":300,"elapsed":22211,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"3f549917-fb0c-47bf-f4d1-865d876189c8"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.94484"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# Analicemos el modelo de tipo bosque aleatorio\n","dir(model_base_RF)"],"metadata":{"id":"c3N3dMCCVSQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_base_RF.get_params()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jz-qXDg8VsE1","executionInfo":{"status":"ok","timestamp":1740708969508,"user_tz":300,"elapsed":11,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"1a0c3244-5d65-464a-c2ec-92de0805e600"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'bootstrap': True,\n"," 'ccp_alpha': 0.0,\n"," 'class_weight': None,\n"," 'criterion': 'gini',\n"," 'max_depth': None,\n"," 'max_features': 'sqrt',\n"," 'max_leaf_nodes': None,\n"," 'max_samples': None,\n"," 'min_impurity_decrease': 0.0,\n"," 'min_samples_leaf': 1,\n"," 'min_samples_split': 2,\n"," 'min_weight_fraction_leaf': 0.0,\n"," 'monotonic_cst': None,\n"," 'n_estimators': 100,\n"," 'n_jobs': None,\n"," 'oob_score': False,\n"," 'random_state': None,\n"," 'verbose': 0,\n"," 'warm_start': False}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# Implementemos una funcion que nos permita instanciar la clase RandomForestClassifier modificando\n","# el numero de estimadores (numero de arbolitos)\n","# Tambien permitamos que la funcion modifique el porcentaje de datos de testeo\n","\n","def RF_models(num_arboles, ts = 0.25):\n","  # Primer paso : Particionamos los datos\n","  X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = ts)\n","\n","  # Segundo Paso : Instanceamos la clase RandomForestClassifier\n","  model_base_RF = RandomForestClassifier(n_estimators= num_arboles)\n","\n","  # Tercer Paso : Ajuste del modelo\n","  model_base_RF.fit(X_train, y_train)\n","\n","  # Cuarto Paso : Etapa1\n","  y_forecast_base_RF = model_base_RF.predict(X_test)\n","\n","  # Cuarto Paso : Etapa2\n","  acc_base_RF = metrics.accuracy_score(y_test, y_forecast_base_RF)\n","  return acc_base_RF\n","\n"],"metadata":{"id":"-MmcQepKV31s","executionInfo":{"status":"ok","timestamp":1740709385159,"user_tz":300,"elapsed":47,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["RF_models(150)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xrd2Ry_uWYXb","executionInfo":{"status":"ok","timestamp":1740709346323,"user_tz":300,"elapsed":43433,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"93fd4520-276d-4383-b491-c29fbf3ef8d1"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.94996"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["RF_models(250, 0.3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tA5M-qKNXCNL","executionInfo":{"status":"ok","timestamp":1740709437159,"user_tz":300,"elapsed":46827,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"fef82806-1703-4a12-ef42-94b8d9dc96d8"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9469"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["import sklearn.ensemble\n","dir(sklearn.ensemble)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXWrJyRSYRYR","executionInfo":{"status":"ok","timestamp":1740709660723,"user_tz":300,"elapsed":12,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"8f4b05ba-5c5c-4c94-adca-7e7000d6fd78"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['AdaBoostClassifier',\n"," 'AdaBoostRegressor',\n"," 'BaggingClassifier',\n"," 'BaggingRegressor',\n"," 'BaseEnsemble',\n"," 'ExtraTreesClassifier',\n"," 'ExtraTreesRegressor',\n"," 'GradientBoostingClassifier',\n"," 'GradientBoostingRegressor',\n"," 'HistGradientBoostingClassifier',\n"," 'HistGradientBoostingRegressor',\n"," 'IsolationForest',\n"," 'RandomForestClassifier',\n"," 'RandomForestRegressor',\n"," 'RandomTreesEmbedding',\n"," 'StackingClassifier',\n"," 'StackingRegressor',\n"," 'VotingClassifier',\n"," 'VotingRegressor',\n"," '__all__',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_bagging',\n"," '_base',\n"," '_forest',\n"," '_gb',\n"," '_gradient_boosting',\n"," '_hist_gradient_boosting',\n"," '_iforest',\n"," '_stacking',\n"," '_voting',\n"," '_weight_boosting']"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["help(sklearn.ensemble.ExtraTreesClassifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34r_a4t2Y68u","executionInfo":{"status":"ok","timestamp":1740709819489,"user_tz":300,"elapsed":81,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"fd9d9118-5318-4bea-ff0a-1fa207615504"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class ExtraTreesClassifier in module sklearn.ensemble._forest:\n","\n","class ExtraTreesClassifier(ForestClassifier)\n"," |  ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n"," |  \n"," |  An extra-trees classifier.\n"," |  \n"," |  This class implements a meta estimator that fits a number of\n"," |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n"," |  of the dataset and uses averaging to improve the predictive accuracy\n"," |  and control over-fitting.\n"," |  \n"," |  Read more in the :ref:`User Guide <forest>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_estimators : int, default=100\n"," |      The number of trees in the forest.\n"," |  \n"," |      .. versionchanged:: 0.22\n"," |         The default value of ``n_estimators`` changed from 10 to 100\n"," |         in 0.22.\n"," |  \n"," |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n"," |      The function to measure the quality of a split. Supported criteria are\n"," |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n"," |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n"," |      Note: This parameter is tree-specific.\n"," |  \n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |  \n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |  \n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |  \n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |  \n"," |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n"," |      The number of features to consider when looking for the best split:\n"," |  \n"," |      - If int, then consider `max_features` features at each split.\n"," |      - If float, then `max_features` is a fraction and\n"," |        `max(1, int(max_features * n_features_in_))` features are considered at each\n"," |        split.\n"," |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |      - If \"log2\", then `max_features=log2(n_features)`.\n"," |      - If None, then `max_features=n_features`.\n"," |  \n"," |      .. versionchanged:: 1.1\n"," |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n"," |  \n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |  \n"," |  max_leaf_nodes : int, default=None\n"," |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |  \n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |  \n"," |      The weighted impurity decrease equation is the following::\n"," |  \n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |  \n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |  \n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |  \n"," |  bootstrap : bool, default=False\n"," |      Whether bootstrap samples are used when building trees. If False, the\n"," |      whole dataset is used to build each tree.\n"," |  \n"," |  oob_score : bool or callable, default=False\n"," |      Whether to use out-of-bag samples to estimate the generalization score.\n"," |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n"," |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n"," |      custom metric. Only available if `bootstrap=True`.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n"," |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n"," |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors. See :term:`Glossary\n"," |      <n_jobs>` for more details.\n"," |  \n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls 3 sources of randomness:\n"," |  \n"," |      - the bootstrapping of the samples used when building trees\n"," |        (if ``bootstrap=True``)\n"," |      - the sampling of the features to consider when looking for the best\n"," |        split at each node (if ``max_features < n_features``)\n"," |      - the draw of the splits for each of the `max_features`\n"," |  \n"," |      See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  verbose : int, default=0\n"," |      Controls the verbosity when fitting and predicting.\n"," |  \n"," |  warm_start : bool, default=False\n"," |      When set to ``True``, reuse the solution of the previous call to fit\n"," |      and add more estimators to the ensemble, otherwise, just fit a whole\n"," |      new forest. See :term:`Glossary <warm_start>` and\n"," |      :ref:`tree_ensemble_warm_start` for details.\n"," |  \n"," |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If not given, all classes are supposed to have weight one. For\n"," |      multi-output problems, a list of dicts can be provided in the same\n"," |      order as the columns of y.\n"," |  \n"," |      Note that for multioutput (including multilabel) weights should be\n"," |      defined for each class of every column in its own dict. For example,\n"," |      for four-class multilabel classification weights should be\n"," |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n"," |      [{1:1}, {2:5}, {3:1}, {4:1}].\n"," |  \n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``\n"," |  \n"," |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n"," |      weights are computed based on the bootstrap sample for every tree\n"," |      grown.\n"," |  \n"," |      For multi-output, the weights of each column of y will be multiplied.\n"," |  \n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |  \n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details. See\n"," |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n"," |      for an example of such pruning.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  max_samples : int or float, default=None\n"," |      If bootstrap is True, the number of samples to draw from X\n"," |      to train each base estimator.\n"," |  \n"," |      - If None (default), then draw `X.shape[0]` samples.\n"," |      - If int, then draw `max_samples` samples.\n"," |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n"," |        `max_samples` should be in the interval `(0.0, 1.0]`.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  monotonic_cst : array-like of int of shape (n_features), default=None\n"," |      Indicates the monotonicity constraint to enforce on each feature.\n"," |        - 1: monotonically increasing\n"," |        - 0: no constraint\n"," |        - -1: monotonically decreasing\n"," |  \n"," |      If monotonic_cst is None, no constraints are applied.\n"," |  \n"," |      Monotonicity constraints are not supported for:\n"," |        - multiclass classifications (i.e. when `n_classes > 2`),\n"," |        - multioutput classifications (i.e. when `n_outputs_ > 1`),\n"," |        - classifications trained on data with missing values.\n"," |  \n"," |      The constraints hold over the probability of the positive class.\n"," |  \n"," |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n"," |  \n"," |      .. versionadded:: 1.4\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  estimator_ : :class:`~sklearn.tree.ExtraTreeClassifier`\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |      .. versionadded:: 1.2\n"," |         `base_estimator_` was renamed to `estimator_`.\n"," |  \n"," |  estimators_ : list of DecisionTreeClassifier\n"," |      The collection of fitted sub-estimators.\n"," |  \n"," |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n"," |      The classes labels (single output problem), or a list of arrays of\n"," |      class labels (multi-output problem).\n"," |  \n"," |  n_classes_ : int or list\n"," |      The number of classes (single output problem), or a list containing the\n"," |      number of classes for each output (multi-output problem).\n"," |  \n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |  \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |  \n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |  \n"," |      .. versionadded:: 0.24\n"," |  \n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |  \n"," |      .. versionadded:: 1.0\n"," |  \n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |  \n"," |  oob_score_ : float\n"," |      Score of the training dataset obtained using an out-of-bag estimate.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n"," |      Decision function computed with out-of-bag estimate on the training\n"," |      set. If n_estimators is small it might be possible that a data point\n"," |      was never left out during the bootstrap. In this case,\n"," |      `oob_decision_function_` might contain NaN. This attribute exists\n"," |      only when ``oob_score`` is True.\n"," |  \n"," |  estimators_samples_ : list of arrays\n"," |      The subset of drawn samples (i.e., the in-bag samples) for each base\n"," |      estimator. Each subset is defined by an array of the indices selected.\n"," |  \n"," |      .. versionadded:: 1.4\n"," |  \n"," |  See Also\n"," |  --------\n"," |  ExtraTreesRegressor : An extra-trees regressor with random splits.\n"," |  RandomForestClassifier : A random forest classifier with optimal splits.\n"," |  RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |  \n"," |  References\n"," |  ----------\n"," |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n"," |         trees\", Machine Learning, 63(1), 3-42, 2006.\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.ensemble import ExtraTreesClassifier\n"," |  >>> from sklearn.datasets import make_classification\n"," |  >>> X, y = make_classification(n_features=4, random_state=0)\n"," |  >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n"," |  >>> clf.fit(X, y)\n"," |  ExtraTreesClassifier(random_state=0)\n"," |  >>> clf.predict([[0, 0, 0, 0]])\n"," |  array([1])\n"," |  \n"," |  Method resolution order:\n"," |      ExtraTreesClassifier\n"," |      ForestClassifier\n"," |      sklearn.base.ClassifierMixin\n"," |      BaseForest\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.ensemble._base.BaseEnsemble\n"," |      sklearn.base.MetaEstimatorMixin\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n"," |      sklearn.utils._metadata_requests._MetadataRequester\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  set_fit_request(self: sklearn.ensemble._forest.ExtraTreesClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``fit`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_score_request(self: sklearn.ensemble._forest.ExtraTreesClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.ExtraTreesClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``score`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``score``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from ForestClassifier:\n"," |  \n"," |  __sklearn_tags__(self)\n"," |  \n"," |  predict(self, X)\n"," |      Predict class for X.\n"," |      \n"," |      The predicted class of an input sample is a vote by the trees in\n"," |      the forest, weighted by their probability estimates. That is,\n"," |      the predicted class is the one with highest mean probability\n"," |      estimate across the trees.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted classes.\n"," |  \n"," |  predict_log_proba(self, X)\n"," |      Predict class log-probabilities for X.\n"," |      \n"," |      The predicted class log-probabilities of an input sample is computed as\n"," |      the log of the mean predicted class probabilities of the trees in the\n"," |      forest.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  predict_proba(self, X)\n"," |      Predict class probabilities for X.\n"," |      \n"," |      The predicted class probabilities of an input sample are computed as\n"," |      the mean predicted class probabilities of the trees in the forest.\n"," |      The class probability of a single tree is the fraction of samples of\n"," |      the same class in a leaf.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseForest:\n"," |  \n"," |  apply(self, X)\n"," |      Apply trees in the forest to X, return leaf indices.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : ndarray of shape (n_samples, n_estimators)\n"," |          For each datapoint x in X and for each tree in the forest,\n"," |          return the index of the leaf x ends up in.\n"," |  \n"," |  decision_path(self, X)\n"," |      Return the decision path in the forest.\n"," |      \n"," |      .. versionadded:: 0.18\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator matrix where non zero elements indicates\n"," |          that the samples goes through the nodes. The matrix is of CSR\n"," |          format.\n"," |      \n"," |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n"," |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n"," |          gives the indicator value for the i-th estimator.\n"," |  \n"," |  fit(self, X, y, sample_weight=None)\n"," |      Build a forest of trees from the training set (X, y).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, its dtype will be converted\n"," |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels in classification, real numbers in\n"," |          regression).\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. In the case of\n"," |          classification, splits are also ignored if they would result in any\n"," |          single class carrying a negative weight in either child node.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          Fitted estimator.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from BaseForest:\n"," |  \n"," |  estimators_samples_\n"," |      The subset of drawn samples for each base estimator.\n"," |      \n"," |      Returns a dynamically generated list of indices identifying\n"," |      the samples used for fitting each member of the ensemble, i.e.,\n"," |      the in-bag samples.\n"," |      \n"," |      Note: the list is re-created at each call to the property in order\n"," |      to reduce the object memory footprint by not storing the sampling\n"," |      data. Thus fetching the property may be slower than expected.\n"," |  \n"," |  feature_importances_\n"," |      The impurity-based feature importances.\n"," |      \n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |      \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          The values of this array sum to 1, unless all trees are single node\n"," |          trees consisting of only the root node, in which case it will be an\n"," |          array of zeros.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  __getitem__(self, index)\n"," |      Return the index'th estimator in the ensemble.\n"," |  \n"," |  __iter__(self)\n"," |      Return iterator over estimators in the ensemble.\n"," |  \n"," |  __len__(self)\n"," |      Return the number of estimators in the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |      Helper for pickle.\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  __sklearn_clone__(self)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  get_metadata_routing(self)\n"," |      Get metadata routing of this object.\n"," |      \n"," |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      routing : MetadataRequest\n"," |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n"," |          routing information.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  __init_subclass__(**kwargs)\n"," |      Set the ``set_{method}_request`` methods.\n"," |      \n"," |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n"," |      looks for the information available in the set default values which are\n"," |      set using ``__metadata_request__*`` class attributes, or inferred\n"," |      from method signatures.\n"," |      \n"," |      The ``__metadata_request__*`` class attributes are used when a method\n"," |      does not explicitly accept a metadata through its arguments or if the\n"," |      developer would like to specify a request value for those metadata\n"," |      which are different from the default ``None``.\n"," |      \n"," |      References\n"," |      ----------\n"," |      .. [1] https://www.python.org/dev/peps/pep-0487\n","\n"]}]},{"cell_type":"code","source":["# XgBoost\n","import xgboost"],"metadata":{"id":"SP7V6xCHZ-Hp","executionInfo":{"status":"ok","timestamp":1740710092883,"user_tz":300,"elapsed":617,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Documentacion del modulo xgboost\n","help(xgboost)"],"metadata":{"id":"kx1nqpU1aC3q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#\n","dir(xgboost)\n","\n","# Para problemas de clasificacion : XGBClassifier y XGBRFClassifier"],"metadata":{"id":"GWZUmPfsaKI0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["help(xgboost.XGBClassifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oNKGtXdSaV6v","executionInfo":{"status":"ok","timestamp":1740710187787,"user_tz":300,"elapsed":142,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"ff718b1c-89ab-43fb-ddf3-c58632e0fa8b"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class XGBClassifier in module xgboost.sklearn:\n","\n","class XGBClassifier(sklearn.base.ClassifierMixin, XGBModel)\n"," |  XGBClassifier(*, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n"," |  \n"," |  Implementation of the scikit-learn API for XGBoost classification.\n"," |  See :doc:`/python/sklearn_estimator` for more information.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  \n"," |      n_estimators : Optional[int]\n"," |          Number of boosting rounds.\n"," |  \n"," |      max_depth :  typing.Optional[int]\n"," |  \n"," |          Maximum tree depth for base learners.\n"," |  \n"," |      max_leaves : typing.Optional[int]\n"," |  \n"," |          Maximum number of leaves; 0 indicates no limit.\n"," |  \n"," |      max_bin : typing.Optional[int]\n"," |  \n"," |          If using histogram-based algorithm, maximum number of bins per feature\n"," |  \n"," |      grow_policy : typing.Optional[str]\n"," |  \n"," |          Tree growing policy.\n"," |  \n"," |          - depthwise: Favors splitting at nodes closest to the node,\n"," |          - lossguide: Favors splitting at nodes with highest loss change.\n"," |  \n"," |      learning_rate : typing.Optional[float]\n"," |  \n"," |          Boosting learning rate (xgb's \"eta\")\n"," |  \n"," |      verbosity : typing.Optional[int]\n"," |  \n"," |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n"," |  \n"," |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n"," |  \n"," |          Specify the learning task and the corresponding learning objective or a custom\n"," |          objective function to be used.\n"," |  \n"," |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n"," |          :ref:`custom-obj-metric` for more information, along with the end note for\n"," |          function signatures.\n"," |  \n"," |      booster: typing.Optional[str]\n"," |  \n"," |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n"," |  \n"," |      tree_method : typing.Optional[str]\n"," |  \n"," |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n"," |          default, XGBoost will choose the most conservative option available.  It's\n"," |          recommended to study this option from the parameters document :doc:`tree method\n"," |          </treemethod>`\n"," |  \n"," |      n_jobs : typing.Optional[int]\n"," |  \n"," |          Number of parallel threads used to run xgboost.  When used with other\n"," |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n"," |          parallelize and balance the threads.  Creating thread contention will\n"," |          significantly slow down both algorithms.\n"," |  \n"," |      gamma : typing.Optional[float]\n"," |  \n"," |          (min_split_loss) Minimum loss reduction required to make a further partition on\n"," |          a leaf node of the tree.\n"," |  \n"," |      min_child_weight : typing.Optional[float]\n"," |  \n"," |          Minimum sum of instance weight(hessian) needed in a child.\n"," |  \n"," |      max_delta_step : typing.Optional[float]\n"," |  \n"," |          Maximum delta step we allow each tree's weight estimation to be.\n"," |  \n"," |      subsample : typing.Optional[float]\n"," |  \n"," |          Subsample ratio of the training instance.\n"," |  \n"," |      sampling_method : typing.Optional[str]\n"," |  \n"," |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n"," |  \n"," |          - ``uniform``: Select random training instances uniformly.\n"," |          - ``gradient_based``: Select random training instances with higher probability\n"," |              when the gradient and hessian are larger. (cf. CatBoost)\n"," |  \n"," |      colsample_bytree : typing.Optional[float]\n"," |  \n"," |          Subsample ratio of columns when constructing each tree.\n"," |  \n"," |      colsample_bylevel : typing.Optional[float]\n"," |  \n"," |          Subsample ratio of columns for each level.\n"," |  \n"," |      colsample_bynode : typing.Optional[float]\n"," |  \n"," |          Subsample ratio of columns for each split.\n"," |  \n"," |      reg_alpha : typing.Optional[float]\n"," |  \n"," |          L1 regularization term on weights (xgb's alpha).\n"," |  \n"," |      reg_lambda : typing.Optional[float]\n"," |  \n"," |          L2 regularization term on weights (xgb's lambda).\n"," |  \n"," |      scale_pos_weight : typing.Optional[float]\n"," |          Balancing of positive and negative weights.\n"," |  \n"," |      base_score : typing.Optional[float]\n"," |  \n"," |          The initial prediction score of all instances, global bias.\n"," |  \n"," |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n"," |  \n"," |          Random number seed.\n"," |  \n"," |          .. note::\n"," |  \n"," |             Using gblinear booster with shotgun updater is nondeterministic as\n"," |             it uses Hogwild algorithm.\n"," |  \n"," |      missing : float\n"," |  \n"," |          Value in the data which needs to be present as a missing value. Default to\n"," |          :py:data:`numpy.nan`.\n"," |  \n"," |      num_parallel_tree: typing.Optional[int]\n"," |  \n"," |          Used for boosting random forest.\n"," |  \n"," |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n"," |  \n"," |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n"," |          for more information.\n"," |  \n"," |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n"," |  \n"," |          Constraints for interaction representing permitted interactions.  The\n"," |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n"," |          3, 4]]``, where each inner list is a group of indices of features that are\n"," |          allowed to interact with each other.  See :doc:`tutorial\n"," |          </tutorials/feature_interaction_constraint>` for more information\n"," |  \n"," |      importance_type: typing.Optional[str]\n"," |  \n"," |          The feature importance type for the feature_importances\\_ property:\n"," |  \n"," |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n"," |            \"total_cover\".\n"," |          * For linear model, only \"weight\" is defined and it's the normalized\n"," |            coefficients without bias.\n"," |  \n"," |      device : typing.Optional[str]\n"," |  \n"," |          .. versionadded:: 2.0.0\n"," |  \n"," |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n"," |  \n"," |      validate_parameters : typing.Optional[bool]\n"," |  \n"," |          Give warnings for unknown parameter.\n"," |  \n"," |      enable_categorical : bool\n"," |  \n"," |          See the same parameter of :py:class:`DMatrix` for details.\n"," |  \n"," |      feature_types : typing.Optional[typing.Sequence[str]]\n"," |  \n"," |          .. versionadded:: 1.7.0\n"," |  \n"," |          Used for specifying feature types without constructing a dataframe. See\n"," |          :py:class:`DMatrix` for details.\n"," |  \n"," |      max_cat_to_onehot : typing.Optional[int]\n"," |  \n"," |          .. versionadded:: 1.6.0\n"," |  \n"," |          .. note:: This parameter is experimental\n"," |  \n"," |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n"," |          for categorical data.  When number of categories is lesser than the threshold\n"," |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n"," |          into children nodes. Also, `enable_categorical` needs to be set to have\n"," |          categorical feature support. See :doc:`Categorical Data\n"," |          </tutorials/categorical>` and :ref:`cat-param` for details.\n"," |  \n"," |      max_cat_threshold : typing.Optional[int]\n"," |  \n"," |          .. versionadded:: 1.7.0\n"," |  \n"," |          .. note:: This parameter is experimental\n"," |  \n"," |          Maximum number of categories considered for each split. Used only by\n"," |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n"," |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n"," |          </tutorials/categorical>` and :ref:`cat-param` for details.\n"," |  \n"," |      multi_strategy : typing.Optional[str]\n"," |  \n"," |          .. versionadded:: 2.0.0\n"," |  \n"," |          .. note:: This parameter is working-in-progress.\n"," |  \n"," |          The strategy used for training multi-target models, including multi-target\n"," |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n"," |          more information.\n"," |  \n"," |          - ``one_output_per_tree``: One model for each target.\n"," |          - ``multi_output_tree``:  Use multi-target trees.\n"," |  \n"," |      eval_metric : typing.Union[str, typing.List[str], typing.Callable, NoneType]\n"," |  \n"," |          .. versionadded:: 1.6.0\n"," |  \n"," |          Metric used for monitoring the training result and early stopping.  It can be a\n"," |          string or list of strings as names of predefined metric in XGBoost (See\n"," |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any\n"," |          other user defined metric that looks like `sklearn.metrics`.\n"," |  \n"," |          If custom objective is also provided, then custom metric should implement the\n"," |          corresponding reverse link function.\n"," |  \n"," |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n"," |          object is provided, it's assumed to be a cost function and by default XGBoost\n"," |          will minimize the result during early stopping.\n"," |  \n"," |          For advanced usage on Early stopping like directly choosing to maximize instead\n"," |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n"," |  \n"," |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n"," |          information.\n"," |  \n"," |          .. code-block:: python\n"," |  \n"," |              from sklearn.datasets import load_diabetes\n"," |              from sklearn.metrics import mean_absolute_error\n"," |              X, y = load_diabetes(return_X_y=True)\n"," |              reg = xgb.XGBRegressor(\n"," |                  tree_method=\"hist\",\n"," |                  eval_metric=mean_absolute_error,\n"," |              )\n"," |              reg.fit(X, y, eval_set=[(X, y)])\n"," |  \n"," |      early_stopping_rounds : typing.Optional[int]\n"," |  \n"," |          .. versionadded:: 1.6.0\n"," |  \n"," |          - Activates early stopping. Validation metric needs to improve at least once in\n"," |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n"," |            least one item in **eval_set** in :py:meth:`fit`.\n"," |  \n"," |          - If early stopping occurs, the model will have two additional attributes:\n"," |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n"," |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n"," |            number of trees during inference. If users want to access the full model\n"," |            (including trees built after early stopping), they can specify the\n"," |            `iteration_range` in these inference methods. In addition, other utilities\n"," |            like model plotting can also use the entire model.\n"," |  \n"," |          - If you prefer to discard the trees after `best_iteration`, consider using the\n"," |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n"," |  \n"," |          - If there's more than one item in **eval_set**, the last entry will be used for\n"," |            early stopping.  If there's more than one metric in **eval_metric**, the last\n"," |            metric will be used for early stopping.\n"," |  \n"," |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n"," |  \n"," |          List of callback functions that are applied at end of each iteration.\n"," |          It is possible to use predefined callbacks by using\n"," |          :ref:`Callback API <callback_api>`.\n"," |  \n"," |          .. note::\n"," |  \n"," |             States in callback are not preserved during training, which means callback\n"," |             objects can not be reused for multiple training sessions without\n"," |             reinitialization or deepcopy.\n"," |  \n"," |          .. code-block:: python\n"," |  \n"," |              for params in parameters_grid:\n"," |                  # be sure to (re)initialize the callbacks before each run\n"," |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n"," |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n"," |                  reg.fit(X, y)\n"," |  \n"," |      kwargs : typing.Optional[typing.Any]\n"," |  \n"," |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n"," |          can be found :doc:`here </parameter>`.\n"," |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n"," |          dict simultaneously will result in a TypeError.\n"," |  \n"," |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n"," |  \n"," |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n"," |              that parameters passed via this argument will interact properly\n"," |              with scikit-learn.\n"," |  \n"," |          .. note::  Custom objective function\n"," |  \n"," |              A custom objective function can be provided for the ``objective``\n"," |              parameter. In this case, it should have the signature ``objective(y_true,\n"," |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n"," |              -> [grad, hess]``:\n"," |  \n"," |              y_true: array_like of shape [n_samples]\n"," |                  The target values\n"," |              y_pred: array_like of shape [n_samples]\n"," |                  The predicted values\n"," |              sample_weight :\n"," |                  Optional sample weights.\n"," |  \n"," |              grad: array_like of shape [n_samples]\n"," |                  The value of the gradient for each sample point.\n"," |              hess: array_like of shape [n_samples]\n"," |                  The value of the second derivative for each sample point\n"," |  \n"," |  Method resolution order:\n"," |      XGBClassifier\n"," |      sklearn.base.ClassifierMixin\n"," |      XGBModel\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n"," |      sklearn.utils._metadata_requests._MetadataRequester\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, *, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n"," |  \n"," |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBClassifier'\n"," |      Fit gradient boosting classifier.\n"," |      \n"," |      Note that calling ``fit()`` multiple times will cause the model object to be\n"," |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n"," |      pass ``xgb_model`` argument.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X :\n"," |          Feature matrix. See :ref:`py-data` for a list of supported types.\n"," |      \n"," |          When the ``tree_method`` is set to ``hist``, internally, the\n"," |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n"," |          for conserving memory. However, this has performance implications when the\n"," |          device of input data is not matched with algorithm. For instance, if the\n"," |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n"," |          data is first processed on CPU then transferred to GPU.\n"," |      y :\n"," |          Labels\n"," |      sample_weight :\n"," |          instance weights\n"," |      base_margin :\n"," |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n"," |      eval_set :\n"," |          A list of (X, y) tuple pairs to use as validation sets, for which\n"," |          metrics will be computed.\n"," |          Validation metrics will help us track the performance of the model.\n"," |      \n"," |      verbose :\n"," |          If `verbose` is True and an evaluation set is used, the evaluation metric\n"," |          measured on the validation set is printed to stdout at each boosting stage.\n"," |          If `verbose` is an integer, the evaluation metric is printed at each\n"," |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n"," |          by using `early_stopping_rounds` is also printed.\n"," |      xgb_model :\n"," |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n"," |          loaded before training (allows training continuation).\n"," |      sample_weight_eval_set :\n"," |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n"," |          object storing instance weights for the i-th validation set.\n"," |      base_margin_eval_set :\n"," |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n"," |          object storing base margin for the i-th validation set.\n"," |      feature_weights :\n"," |          Weight for each feature, defines the probability of each feature being\n"," |          selected when colsample is being used.  All values must be greater than 0,\n"," |          otherwise a `ValueError` is thrown.\n"," |  \n"," |  predict(self, X: Any, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n"," |      Predict with `X`.  If the model is trained with early stopping, then\n"," |      :py:attr:`best_iteration` is used automatically. The estimator uses\n"," |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n"," |      devices between the data and the estimator don't match.\n"," |      \n"," |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X :\n"," |          Data to predict with.\n"," |      output_margin :\n"," |          Whether to output the raw untransformed margin value.\n"," |      validate_features :\n"," |          When this is True, validate that the Booster's and data's feature_names are\n"," |          identical.  Otherwise, it is assumed that the feature_names are the same.\n"," |      base_margin :\n"," |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n"," |      iteration_range :\n"," |          Specifies which layer of trees are used in prediction.  For example, if a\n"," |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n"," |          20)``, then only the forests built during [10, 20) (half open set) rounds\n"," |          are used in this prediction.\n"," |      \n"," |          .. versionadded:: 1.4.0\n"," |      \n"," |      Returns\n"," |      -------\n"," |      prediction\n"," |  \n"," |  predict_proba(self, X: Any, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n"," |      Predict the probability of each `X` example being of a given class. If the\n"," |      model is trained with early stopping, then :py:attr:`best_iteration` is used\n"," |      automatically. The estimator uses `inplace_predict` by default and falls back to\n"," |      using :py:class:`DMatrix` if devices between the data and the estimator don't\n"," |      match.\n"," |      \n"," |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X :\n"," |          Feature matrix. See :ref:`py-data` for a list of supported types.\n"," |      validate_features :\n"," |          When this is True, validate that the Booster's and data's feature_names are\n"," |          identical.  Otherwise, it is assumed that the feature_names are the same.\n"," |      base_margin :\n"," |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n"," |      iteration_range :\n"," |          Specifies which layer of trees are used in prediction.  For example, if a\n"," |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n"," |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n"," |          used in this prediction.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      prediction :\n"," |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n"," |          probability of each data example being of a given class.\n"," |  \n"," |  set_fit_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``fit`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``base_margin`` parameter in ``fit``.\n"," |      \n"," |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n"," |      \n"," |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``eval_set`` parameter in ``fit``.\n"," |      \n"," |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n"," |      \n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n"," |      \n"," |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n"," |      \n"," |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``verbose`` parameter in ``fit``.\n"," |      \n"," |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_predict_proba_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``predict_proba`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``base_margin`` parameter in ``predict_proba``.\n"," |      \n"," |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``iteration_range`` parameter in ``predict_proba``.\n"," |      \n"," |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``validate_features`` parameter in ``predict_proba``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_predict_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``predict`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``base_margin`` parameter in ``predict``.\n"," |      \n"," |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n"," |      \n"," |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``output_margin`` parameter in ``predict``.\n"," |      \n"," |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``validate_features`` parameter in ``predict``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  set_score_request(self: xgboost.sklearn.XGBClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``score`` method.\n"," |      \n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      The options for each parameter are:\n"," |      \n"," |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n"," |      \n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n"," |      \n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |      \n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |      \n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |      \n"," |      .. versionadded:: 1.3\n"," |      \n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``score``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties defined here:\n"," |  \n"," |  classes_\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __annotations__ = {}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |      \n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from XGBModel:\n"," |  \n"," |  __sklearn_is_fitted__(self) -> bool\n"," |  \n"," |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n"," |      Return the predicted leaf every tree for each sample. If the model is trained\n"," |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array_like, shape=[n_samples, n_features]\n"," |          Input features matrix.\n"," |      \n"," |      iteration_range :\n"," |          See :py:meth:`predict`.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : array_like, shape=[n_samples, n_trees]\n"," |          For each datapoint x in X and for each tree, return the index of the\n"," |          leaf x ends up in. Leaves are numbered within\n"," |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n"," |  \n"," |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n"," |      Return the evaluation results.\n"," |      \n"," |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n"," |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n"," |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n"," |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n"," |      function.\n"," |      \n"," |      The returned evaluation result is a dictionary:\n"," |      \n"," |      .. code-block:: python\n"," |      \n"," |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n"," |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n"," |      \n"," |      Returns\n"," |      -------\n"," |      evals_result\n"," |  \n"," |  get_booster(self) -> xgboost.core.Booster\n"," |      Get the underlying xgboost Booster of this model.\n"," |      \n"," |      This will raise an exception when fit was not called\n"," |      \n"," |      Returns\n"," |      -------\n"," |      booster : a xgboost booster of underlying model\n"," |  \n"," |  get_num_boosting_rounds(self) -> int\n"," |      Gets the number of xgboost boosting rounds.\n"," |  \n"," |  get_params(self, deep: bool = True) -> Dict[str, Any]\n"," |      Get parameters.\n"," |  \n"," |  get_xgb_params(self) -> Dict[str, Any]\n"," |      Get xgboost specific parameters.\n"," |  \n"," |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n"," |      Load the model from a file or a bytearray.\n"," |      \n"," |      The model is saved in an XGBoost internal format which is universal among the\n"," |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n"," |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n"," |      format. See :doc:`Model IO </tutorials/saving_model>` for more info.\n"," |      \n"," |      .. code-block:: python\n"," |      \n"," |        model.load_model(\"model.json\")\n"," |        # or\n"," |        model.load_model(\"model.ubj\")\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname :\n"," |          Input file name or memory buffer(see also save_raw)\n"," |  \n"," |  save_model(self, fname: Union[str, os.PathLike]) -> None\n"," |      Save the model to a file.\n"," |      \n"," |      The model is saved in an XGBoost internal format which is universal among the\n"," |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n"," |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n"," |      format. See :doc:`Model IO </tutorials/saving_model>` for more info.\n"," |      \n"," |      .. code-block:: python\n"," |      \n"," |        model.save_model(\"model.json\")\n"," |        # or\n"," |        model.save_model(\"model.ubj\")\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      fname :\n"," |          Output file name\n"," |  \n"," |  set_params(self, **params: Any) -> 'XGBModel'\n"," |      Set the parameters of this estimator.  Modification of the sklearn method to\n"," |      allow unknown kwargs. This allows using the full range of xgboost\n"," |      parameters that are not defined as member variables in sklearn grid\n"," |      search.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from XGBModel:\n"," |  \n"," |  best_iteration\n"," |      The best iteration obtained by early stopping.  This attribute is 0-based,\n"," |      for instance if the best iteration is the first round, then best_iteration is 0.\n"," |  \n"," |  best_score\n"," |      The best score obtained by early stopping.\n"," |  \n"," |  coef_\n"," |      Coefficients property\n"," |      \n"," |      .. note:: Coefficients are defined only for linear learners\n"," |      \n"," |          Coefficients are only defined when the linear model is chosen as\n"," |          base learner (`booster=gblinear`). It is not defined for other base\n"," |          learner types, such as tree learners (`booster=gbtree`).\n"," |      \n"," |      Returns\n"," |      -------\n"," |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n"," |  \n"," |  feature_importances_\n"," |      Feature importances property, return depends on `importance_type`\n"," |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n"," |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n"," |      based on the importance type. For instance, if the importance type is\n"," |      \"total_gain\", then the score is sum of loss change for each split from all\n"," |      trees.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n"," |      linear model, which returns an array with shape `(n_features, n_classes)`\n"," |  \n"," |  feature_names_in_\n"," |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n"," |      feature names that are all strings.\n"," |  \n"," |  intercept_\n"," |      Intercept (bias) property\n"," |      \n"," |      For tree-based model, the returned value is the `base_score`.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n"," |  \n"," |  n_features_in_\n"," |      Number of features seen during :py:meth:`fit`.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |      Helper for pickle.\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  __sklearn_clone__(self)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  get_metadata_routing(self)\n"," |      Get metadata routing of this object.\n"," |      \n"," |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      routing : MetadataRequest\n"," |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n"," |          routing information.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |  \n"," |  __init_subclass__(**kwargs)\n"," |      Set the ``set_{method}_request`` methods.\n"," |      \n"," |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n"," |      looks for the information available in the set default values which are\n"," |      set using ``__metadata_request__*`` class attributes, or inferred\n"," |      from method signatures.\n"," |      \n"," |      The ``__metadata_request__*`` class attributes are used when a method\n"," |      does not explicitly accept a metadata through its arguments or if the\n"," |      developer would like to specify a request value for those metadata\n"," |      which are different from the default ``None``.\n"," |      \n"," |      References\n"," |      ----------\n"," |      .. [1] https://www.python.org/dev/peps/pep-0487\n","\n"]}]}]}